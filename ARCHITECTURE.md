# CausalFlow Technical Specification & Architecture

TÃ i liá»‡u nÃ y cung cáº¥p cÃ¡i nhÃ¬n chuyÃªn sÃ¢u vá» kiáº¿n trÃºc ná»™i bá»™, cÃ¡c thÃ nh pháº§n toÃ¡n há»c vÃ  quy trÃ¬nh xá»­ lÃ½ cá»§a framework **CausalFlow**.

---

## 1. Cáº¥u trÃºc MÃ´ hÃ¬nh PhÃ¢n lá»›p (Layered Architecture)

CausalFlow Ä‘Æ°á»£c thiáº¿t káº¿ theo cáº¥u trá»¥c phÃ¢n lá»›p Ä‘á»ƒ tÃ¡ch biá»‡t giá»¯a viá»‡c trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng, mÃ´ hÃ¬nh hÃ³a nhiá»…u vÃ  tá»‘i Æ°u hÃ³a cáº¥u trÃºc Ä‘á»“ thá»‹.

```mermaid
graph TD
    subgraph Input_Layer [MÃ´ Ä‘un Tiá»n xá»­ lÃ½]
        I[Raw Data] --> QT[Quantile Transformer: Gaussianizing]
        QT --> IF[Isolation Forest: Outlier Filtration]
    end

    subgraph Feature_Extraction [Core Backbone - MLP Module]
        IF --> ATT[Self-Attention: Feature Weighting]
        ATT --> RB[ResNet Blocks: Deep Processing]
        RB --> GRN[Gated Residual Networks: Input Gating]
    end

    subgraph Causal_Discovery [Causal Engine - GPPOM Module]
        GRN --> VAE[VAE Head: Latent Mechanism Z]
        GRN --> NSF[Neural Spline Flows: Noise H]
        GRN --> NT[NOTEARS: Adjacency Matrix W]
    end

    subgraph Inference_Layer [Statistical Testing]
        NSF & VAE --> GP[Gaussian Process Head: Prediction]
        GP --> RES[Residual Extraction]
        RES --> HSIC[HSIC: Independence Validation]
    end

    HSIC --> OUT[Bail: Causal Direction / DAG]
```

---

## 2. Chi tiáº¿t cÃ¡c thÃ nh pháº§n SOTA (State-of-the-Art Components)

### 2.1. Neural Spline Flows (MÃ´ hÃ¬nh hÃ³a Nhiá»…u)
Thay vÃ¬ giáº£ Ä‘á»‹nh nhiá»…u lÃ  dáº¡ng Gaussian Ä‘Æ¡n giáº£n, dá»± Ã¡n sá»­ dá»¥ng **Monotonic Spline Layers**.
*   **CÆ¡ cháº¿:** Sá»­ dá»¥ng cÃ¡c hÃ m Spline Ä‘Æ¡n Ä‘iá»‡u báº­c ba Ä‘á»ƒ thá»±c hiá»‡n phÃ©p biáº¿n Ä‘á»•i $h(Y)$.
*   **Æ¯u Ä‘iá»ƒm:** Cho phÃ©p mÃ´ hÃ¬nh hÃ³a cÃ¡c phÃ¢n phá»‘i nhiá»…u cá»±c ká»³ phá»©c táº¡p (Multi-modal, Heavy-tailed) mÃ  váº«n Ä‘áº£m báº£o tÃ­nh kháº£ nghá»‹ch (invertibility) Ä‘á»ƒ trÃ­ch xuáº¥t sáº¡ch nhiá»…u.

### 2.2. Differentiable DAG Learning (NOTEARS)
Sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a liÃªn tá»¥c trÃªn ma tráº­n trá»ng sá»‘ $W$.
*   **HÃ m rÃ ng buá»™c:** $h(W) = Tr(e^{W \circ W}) - d = 0$.
*   **Má»¥c tiÃªu:** Ã‰p ma tráº­n ká» pháº£i lÃ  Ä‘á»“ thá»‹ khÃ´ng vÃ²ng (Directed Acyclic Graph) thÃ´ng qua Gradient Descent, giÃºp trÃ¡nh viá»‡c pháº£i tÃ¬m kiáº¿m tá»• há»£p (combinatorial search) tá»‘n kÃ©m.

### 2.3. Fast HSIC via Random Fourier Features (RFF)
Äá»ƒ tÄƒng tá»‘c phÃ©p thá»­ Ä‘á»™c láº­p thá»‘ng kÃª tá»« $O(N^2)$ xuá»‘ng $O(N)$:
*   **CÆ¡ cháº¿:** Ãnh xáº¡ dá»¯ liá»‡u vÃ o khÃ´ng gian Ä‘áº·c trÆ°ng RKHS sá»­ dá»¥ng cÃ¡c hÃ m Sine/Cosine ngáº«u nhiÃªn.
*   **á»¨ng dá»¥ng:** TÃ­nh toÃ¡n sá»± Ä‘á»™c láº­p giá»¯a pháº§n dÆ° vÃ  nguyÃªn nhÃ¢n trong thá»i gian thá»±c ngay khi huáº¥n luyá»‡n.

---

## 3. Quy trÃ¬nh thá»±c thi cÃ¡c Module (File-level Flow)

### ğŸ“‚ `causalflow/core/`
*   **`mlp.py`**: Chá»©a lá»›p `MLP` Ä‘a Ä‘áº§u ra. NÃ³ khÃ´ng chá»‰ dá»± bÃ¡o giÃ¡ trá»‹ mÃ  cÃ²n trÃ­ch xuáº¥t tham sá»‘ cá»§a phÃ¢n phá»‘i tiá»m áº©n vÃ  thá»±c hiá»‡n phÃ©p biáº¿n Ä‘á»•i PNL (Post-Nonlinear).
*   **`gppom_hsic.py`**: Quáº£n lÃ½ `GPPOMC_lnhsic_Core`. ÄÃ¢y lÃ  nÆ¡i "há»£p nháº¥t" káº¿t quáº£ tá»« MLP vá»›i rÃ ng buá»™c DAG NOTEARS. NÃ³ tÃ­nh toÃ¡n máº¥t mÃ¡t tá»•ng há»£p Ä‘á»ƒ Ä‘iá»u phá»‘i toÃ n bá»™ cÃ¡c thÃ nh pháº§n khÃ¡c.
*   **`kernels.py`**: Äá»‹nh nghÄ©a cÃ¡c Kernel Ä‘áº¡o hÃ m. Kháº£ nÄƒng tá»± há»c (Adaptive) cá»§a mÃ´ hÃ¬nh náº±m á»Ÿ viá»‡c tá»‘i Æ°u hÃ³a `log_gamma` (bÄƒng thÃ´ng) vÃ  `log_alpha` (biÃªn Ä‘á»™) cá»§a cÃ¡c nhÃ¢n nÃ y.

### ğŸ“‚ `causalflow/models/`
*   **`analysis.py`**: Triá»ƒn khai `ANMMM_cd_advanced`.
    1.  Khá»Ÿi táº¡o 2 thá»±c thá»ƒ `CausalFlow`.
    2.  KhÃ³a cá»©ng cáº¥u trÃºc: `W_dag[i,j]=1` cho hÆ°á»›ng thuáº­n vÃ  `W_dag[j,i]=1` cho hÆ°á»›ng nghá»‹ch.
    3.  Äo Ä‘áº¡c Ä‘á»™ Ä‘á»™c láº­p cá»§a pháº§n dÆ° Ä‘á»ƒ Ä‘Æ°a ra káº¿t luáº­n cuá»‘i cÃ¹ng.
*   **`trainer.py`**: Sá»­ dá»¥ng bá»™ tá»‘i Æ°u **AdamW** vá»›i Weight Decay Ä‘á»ƒ trÃ¡nh Overfitting, quáº£n lÃ½ viá»‡c giáº£m nhiá»‡t Ä‘á»™ (temperature) cho lá»›p Gumbel-Softmax.

---

## 4. Äáº·c táº£ luá»“ng dá»¯ liá»‡u (Data Flow Analysis)

```mermaid
sequenceDiagram
    participant D as Data
    participant B as Backbone (ResNet+Attention)
    participant L as Latent (VAE/NSF)
    participant G as GP Head
    participant H as HSIC Test

    D->>B: Input Tensor [Batch, Dim]
    B->>L: Latent Representation
    L->>G: Probabilistic Mapping with DAG Bias
    G->>H: Estimated Residuals
    H->>H: Statistical Independence Check
    H-->>D: Gradient Feedback
```

---

## 5. HÃ m má»¥c tiÃªu tá»‘i Æ°u hÃ³a (Comprehensive Objective)

MÃ´ hÃ¬nh tá»‘i Æ°u hÃ³a hÃ m toÃ¡n há»c cá»±c ká»³ cháº·t cháº½:

$${\cal L}_{total} = {\cal L}_{MSE} + \lambda_{dag} h(W) + \lambda_{hsic} \log(HSIC(X, \hat{\epsilon})) + \lambda_{kl} D_{KL}(q(z)||p(z))$$

*   **MSE:** Äáº£m báº£o kháº£ nÄƒng giáº£i thÃ­ch dá»¯ liá»‡u.
*   **h(W):** Äáº£m báº£o tÃ­nh há»£p lá»‡ cá»§a Ä‘á»“ thá»‹ nhÃ¢n quáº£.
*   **log(HSIC):** Äáº£m báº£o tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a giáº£ thuyáº¿t "NguyÃªn nhÃ¢n Ä‘á»™c láº­p vá»›i Nhiá»…u".
*   **KL:** Äáº£m báº£o cáº¥u trÃºc tiá»m áº©n cá»§a cÆ¡ cháº¿ nhÃ¢n quáº£ khÃ´ng bá»‹ sá»¥p Ä‘á»•.
