# CausalFlow: Deep Neural Causal Discovery Architecture

[![Architecture](https://img.shields.io/badge/Architecture-Detailed_Diagrams-blueviolet?style=flat-square)](ARCH.md)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=flat-square)](LICENSE)

CausalFlow l√† m·ªôt **ki·∫øn tr√∫c m·∫°ng n∆°-ron s√¢u (Deep Neural Architecture)** h·ª£p nh·∫•t, ƒë∆∞·ª£c thi·∫øt k·∫ø chuy√™n bi·ªát cho b√†i to√°n kh√°m ph√° nh√¢n qu·∫£. H·ªá th·ªëng t√≠ch h·ª£p tr·ª±c ti·∫øp c√°c c∆° ch·∫ø m√¥ h√¨nh h√≥a phi tuy·∫øn v√† quy tr√¨nh suy di·ªÖn nh√¢n qu·∫£ v√†o trong m·ªôt m√¥ h√¨nh duy nh·∫•t, gi√∫p t·ªëi ∆∞u h√≥a kh·∫£ nƒÉng nh·∫≠n di·ªán c·∫•u tr√∫c t·ª´ d·ªØ li·ªáu ph·ª©c t·∫°p.

D·ª±a tr√™n n·ªÅn t·∫£ng c·ªßa ph∆∞∆°ng ph√°p ANM-MM, CausalFlow ƒë√≥ng vai tr√≤ l√† m·ªôt engine t√≠nh to√°n m·∫°nh m·∫Ω, k·∫øt h·ª£p gi·ªØa h·ªçc s√¢u v√† c√°c l√Ω thuy·∫øt nh√¢n qu·∫£ hi·ªán ƒë·∫°i.

## üöÄ C·∫£i ti·∫øn so v·ªõi Base (ANM-MM / GPPOM-HSIC)

CausalFlow ƒë√£ n√¢ng c·∫•p n·ªÅn t·∫£ng t·ª´ b·ªô c√¥ng c·ª• `GPPOM-HSIC` c·ªßa amber0309 th√†nh m·ªôt b·ªô khung **Deep Learning Engine** m·∫°nh m·∫Ω v√† h·ª£p nh·∫•t:

| T√≠nh nƒÉng | Base (amber0309) | **CausalFlow (Ours)** | Rationale |
| :--- | :--- | :--- | :--- |
| **Ki·∫øn tr√∫c** | H√†m r·ªùi r·∫°c (Loose scripts) | **Unified Model Class** | ƒê√≥ng g√≥i to√†n b·ªô workflow v√†o m·ªôt Class duy nh·∫•t theo phong c√°ch PyTorch/Scikit-learn. |
| **Neural Backbone** | MLP C∆° b·∫£n (Simple MLP) | **Deep ResNet + GRN + Attention** | TƒÉng kh·∫£ nƒÉng h·ªçc ƒë·∫∑c tr∆∞ng phi tuy·∫øn v√† t·ª± ƒë·ªông lo·∫°i b·ªè bi·∫øn nhi·ªÖu qua Self-Attention. |
| **Noise Modeling** | Gi·∫£ ƒë·ªãnh nhi·ªÖu ƒë∆°n gi·∫£n | **Neural Spline Flows (NSF)** | S·ª≠ d·ª•ng c√°c h√†m Spline c√≥ th·ªÉ ƒë·∫°o h√†m ƒë·ªÉ m√¥ h√¨nh h√≥a c√°c ph√¢n ph·ªëi nhi·ªÖu ph·ª©c t·∫°p. |
| **DAG Learning** | T√¨m ki·∫øm tham lam (Bivariate) | **NOTEARS (Multivariate)** | Kh√°m ph√° c·∫•u tr√∫c ƒë·ªì th·ªã ƒëa bi·∫øn th√¥ng qua t·ªëi ∆∞u h√≥a li√™n t·ª•c, ƒë·∫£m b·∫£o t√≠nh kh√¥ng v√≤ng (Acyclicity). |
| **Inference API** | Kh√¥ng c√≥ s·∫µn | **High-level Analysis API** | T√≠ch h·ª£p s·∫µn `predict_direction`, `predict_counterfactual`, `check_stability` ngay trong model. |
| **Training Flow** | G·ªçi l·ªánh th·ªß c√¥ng | **Auto-Inference & Auto-Train** | T·ª± ƒë·ªông nh·∫≠n di·ªán chi·ªÅu d·ªØ li·ªáu v√† hu·∫•n luy·ªán ngay khi `init` ho·∫∑c g·ªçi `__call__`. |
| **Ti·ªÅn x·ª≠ l√Ω** | C∆° b·∫£n | **Hybrid Preprocessing** | K·∫øt h·ª£p `QuantileTransformer` (Gaussianizing) v√† `Isolation Forest` (Outlier Removal). |

## ƒê·∫∑c ƒëi·ªÉm K·ªπ thu·∫≠t

- **Deep Neural Backbone:** H·ªá th·ªëng s·ª≠ d·ª•ng c√°c kh·ªëi ResNet v√† Gated Residual Networks (GRN) ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu, ƒë·∫£m b·∫£o t√≠nh ·ªïn ƒë·ªãnh v√† kh·∫£ nƒÉng h·ªôi t·ª• cao.
- **Neural Spline Flows (NSF):** M√¥ h√¨nh h√≥a nhi·ªÖu th√¥ng qua c√°c h√†m Spline ƒë∆°n ƒëi·ªáu, gi√∫p tr√≠ch xu·∫•t ph·∫ßn d∆∞ s·∫°ch h∆°n cho c√°c ph√©p th·ª≠ ƒë·ªôc l·∫≠p.
- **Unified Inference API:** Cung c·∫•p c√°c ph∆∞∆°ng th·ª©c c·∫•p cao ƒë·ªÉ th·ª±c hi·ªán ph√¢n t√≠ch ƒë·ªô ·ªïn ƒë·ªãnh (stability) v√† d·ª± b√°o gi·∫£ t∆∞·ªüng (counterfactual) tr·ª±c ti·∫øp t·ª´ m√¥ h√¨nh.
- **Hybrid Loss Function:** T·ªëi ∆∞u h√≥a ƒë·ªìng th·ªùi ƒë·ªô ch√≠nh x√°c d·ª± b√°o (MSE), t√≠nh kh√¥ng v√≤ng c·ªßa ƒë·ªì th·ªã (DAG penalty) v√† t√≠nh ƒë·ªôc l·∫≠p nh√¢n qu·∫£ (HSIC).

## K·∫øt qu·∫£ Th·ª±c nghi·ªám

Hi·ªáu su·∫•t ƒë∆∞·ª£c ki·ªÉm ch·ª©ng tr√™n b·ªô d·ªØ li·ªáu sinh h·ªçc th·ª±c t·∫ø **Sachs**, ƒë·∫°t k·∫øt qu·∫£ v∆∞·ª£t tr·ªôi:

- **ƒê·ªô ch√≠nh x√°c h∆∞·ªõng (Accuracy): 70.6%** (12/17 c·∫°nh ƒë√∫ng).
- **SHD (Structural Hamming Distance): 5**.
- Kh·∫£ nƒÉng x·ª≠ l√Ω phi tuy·∫øn m·∫°nh m·∫Ω, l·ªçc nhi·ªÖu hi·ªáu qu·∫£ b·∫±ng Isolation Forest v√† Quantile Transformation.

### Ch·ªâ s·ªë hi·ªáu nƒÉng so s√°nh

| Thu·∫≠t to√°n | X·ª≠ l√Ω Phi tuy·∫øn | ƒê·ªô ch√≠nh x√°c (Sachs) | SHD | T√≠nh ·ªïn ƒë·ªãnh |
| :--- | :--- | :--- | :--- | :--- |
| **PC Algorithm** | K√©m | ~50-55% | Cao | Th·∫•p |
| **NOTEARS (Original)** | Trung b√¨nh | ~60% | > 8 | Trung b√¨nh |
| **CausalFlow (Ours)** | **R·∫•t t·ªët (NSF)** | **70.6%** | **5** | **Cao** |

## C√†i ƒë·∫∑t

```bash
pip install git+https://github.com/manhthai1706/CausalFlow.git
```

## H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng (Unified API)

### 1. Kh√°m ph√° h∆∞·ªõng nh√¢n qu·∫£ song bi·∫øn (SOTA Pattern)
S·ª≠ d·ª•ng quy tr√¨nh Hypotheses Testing t√≠ch h·ª£p ƒë·ªÉ ƒë·∫°t ƒë·ªô ch√≠nh x√°c cao nh·∫•t:
```python
from causalflow import CausalFlow

# Kh·ªüi t·∫°o v√† d·ª± ƒëo√°n h∆∞·ªõng ngay l·∫≠p t·ª©c (X->Y: 1, Y->X: -1)
model = CausalFlow(lda=12.0)
direction = model.predict_direction(pair_data)
```

### 2. Hu·∫•n luy·ªán v√† Suy di·ªÖn ƒêa bi·∫øn
M√¥ h√¨nh t·ª± ƒë·ªông nh·∫≠n di·ªán chi·ªÅu d·ªØ li·ªáu v√† hu·∫•n luy·ªán:
```python
# C√°ch 1: Train ngay khi kh·ªüi t·∫°o
model = CausalFlow(data=data_matrix, epochs=200)

# C√°ch 2: G·ªçi model nh∆∞ m·ªôt h√†m ƒë·ªÉ train
model = CausalFlow()
model(data_matrix, epochs=200)

# Tr√≠ch xu·∫•t ma tr·∫≠n DAG
W_raw, W_binary = model.get_dag_matrix()
```

### 3. Ph√¢n t√≠ch Gi·∫£ t∆∞·ªüng (Counterfactual)
```python
# D·ª± ƒëo√°n Y s·∫Ω th·∫ø n√†o n·∫øu thay ƒë·ªïi gi√° tr·ªã c·ªßa X
y_cf = model.predict_counterfactual(x_orig, y_orig, x_new)
```

## Tham kh·∫£o

- **ANM-MM (amber0309).** [GitHub Repository](https://github.com/amber0309/ANM-MM). (C∆° s·ªü thu·∫≠t to√°n ban ƒë·∫ßu).
- **Zheng, X., et al. (2018).** "DAGs with NO TEARS: Continuous Optimization for Structure Learning." *NeurIPS*.
- **Durkan, C., et al. (2019).** "Neural Spline Flows." *NeurIPS*.
- **Zhang, K., & Hyvarinen, A. (2009).** "On the Identifiability of the Post-Nonlinear Causal Model." *UAI*.
- **Rahimi, A., & Recht, B. (2007).** "Random Features for Large-Scale Kernel Machines." *NeurIPS*. (T·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô HSIC th√¥ng qua RFF).
- **Gretton, A., et al. (2007).** "A Kernel Statistical Test of Independence." *NeurIPS*. (N·ªÅn t·∫£ng c·ªßa c√°c ph√©p th·ª≠ ƒë·ªôc l·∫≠p HSIC).
- **Vaswani, A., et al. (2017).** "Attention Is All You Need." *NeurIPS*. (C∆° ch·∫ø Self-Attention trong l·ªõp MLP ƒë·ªÉ tr·ªçng s·ªë h√≥a ƒë·∫∑c tr∆∞ng).
- **Jang, E., et al. (2016).** "Categorical Reparameterization with Gumbel-Softmax." *ICLR*. (C∆° ch·∫ø ph√¢n c·ª•m c∆° ch·∫ø nh√¢n qu·∫£ c√≥ th·ªÉ ƒë·∫°o h√†m).
- **Kingma, D. P., & Welling, M. (2013).** "Auto-Encoding Variational Bayes." *ICLR*. (Ki·∫øn tr√∫c VAE ƒë·ªÉ ph√°t hi·ªán c∆° c·∫•u ti·ªÅm ·∫©n).
- **He, K., et al. (2016).** "Deep Residual Learning for Image Recognition." *CVPR*. (C∆° ch·∫ø Residual Connections trong kh·ªëi ResBlock).
- **Ba, J. L., et al. (2016).** "Layer Normalization." *arXiv*. (K·ªπ thu·∫≠t chu·∫©n h√≥a l·ªõp ƒë·ªÉ ·ªïn ƒë·ªãnh qu√° tr√¨nh hu·∫•n luy·ªán).
- **Hendrycks, D., & Gimpel, K. (2016).** "Gaussian Error Linear Units (GELUs)." *arXiv*. (H√†m k√≠ch ho·∫°t GELU trong m√¥ h√¨nh MLP).
- **Lim, B., et al. (2021).** "Temporal Fusion Transformers." *International Journal of Forecasting*. (C·∫•u tr√∫c Gated Residual Network - GRN cho vi·ªác ch·ªçn l·ªçc ƒë·∫∑c tr∆∞ng).
- **Loshchilov, I., & Hutter, F. (2017).** "Decoupled Weight Decay Regularization." *ICLR*. (Thu·∫≠t to√°n t·ªëi ∆∞u AdamW s·ª≠ d·ª•ng trong Trainer).
- **Liu, F. T., et al. (2008).** "Isolation Forest." *ICDM*. (S·ª≠ d·ª•ng l·ªçc Outliers trong ti·ªÅn x·ª≠ l√Ω).
- **Pedregosa, F., et al. (2011).** "Scikit-learn: Machine Learning in Python." *JMLR*. (Cung c·∫•p QuantileTransformer).
- **Paszke, A., et al. (2019).** "PyTorch: An Imperative Style, High-Performance Deep Learning Library." *NeurIPS*.

## License
D·ª± √°n ƒë∆∞·ª£c ph√°t h√†nh d∆∞·ªõi gi·∫•y ph√©p MIT License.
