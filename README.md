# CausalFlow: Unified Deep Neural Engine for Causal Discovery

[![Architecture](https://img.shields.io/badge/Architecture-Detailed_Diagrams-blueviolet?style=flat-square)](ARCH.md)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=flat-square)](LICENSE)

**CausalFlow** l√† m·ªôt h·ªá th·ªëng kh√°m ph√° nh√¢n qu·∫£ (Causal Discovery) ti√™n ti·∫øn, ƒë∆∞·ª£c x√¢y d·ª±ng nh∆∞ m·ªôt c√¥ng c·ª• h·ªçc s√¢u h·ª£p nh·∫•t (Unified Deep Learning Engine). D·ª± √°n t·∫≠p h·ª£p c√°c c√¥ng ngh·ªá SOTA trong m√¥ h√¨nh h√≥a phi tuy·∫øn v√† t·ªëi ∆∞u h√≥a ƒë·ªì th·ªã ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n suy di·ªÖn c·∫•u tr√∫c nh√¢n qu·∫£ t·ª´ d·ªØ li·ªáu quan s√°t ph·ª©c t·∫°p.

Kh√°c v·ªõi c√°c c√¥ng c·ª• truy·ªÅn th·ªëng, CausalFlow ƒë√≥ng g√≥i to√†n b·ªô quy tr√¨nh t·ª´ ti·ªÅn x·ª≠ l√Ω, hu·∫•n luy·ªán c∆° ch·∫ø ƒë·∫øn ph√¢n t√≠ch gi·∫£ t∆∞·ªüng v√†o trong m·ªôt ki·∫øn tr√∫c m·∫°ng n∆°-ron s√¢u duy nh·∫•t, mang l·∫°i hi·ªáu su·∫•t v∆∞·ª£t tr·ªôi v√† s·ª± ti·ªán d·ª•ng t·ªëi ƒëa.

---

## üõ† C√¥ng ngh·ªá & Ki·∫øn tr√∫c C·ªët l√µi

CausalFlow s·ªü h·ªØu m·ªôt "Backbone" k·ªπ thu·∫≠t m·∫°nh m·∫Ω, k·∫øt h·ª£p gi·ªØa h·ªçc s√¢u hi·ªán ƒë·∫°i v√† l√Ω thuy·∫øt nh√¢n qu·∫£:

- **Deep Neural Backbone (ResNet + GRN + Attention):** S·ª≠ d·ª•ng c√°c kh·ªëi ResNet k·∫øt h·ª£p v·ªõi Gated Residual Networks (GRN) v√† c∆° ch·∫ø Self-Attention ƒë·ªÉ t·ª± ƒë·ªông s√†ng l·ªçc ƒë·∫∑c tr∆∞ng, gi√∫p m√¥ h√¨nh nh·∫°y b√©n v·ªõi c√°c t√≠n hi·ªáu nh√¢n qu·∫£ th·ª±c s·ª± v√† lo·∫°i b·ªè bi·∫øn nhi·ªÖu.
- **Neural Spline Flows (NSF):** T√≠ch h·ª£p c√¥ng ngh·ªá Normalizing Flows th√¥ng qua c√°c h√†m Spline ƒë∆°n ƒëi·ªáu ƒë·ªÉ m√¥ h√¨nh h√≥a ph√¢n ph·ªëi nhi·ªÖu phi tuy·∫øn b·∫≠c cao, ƒë·∫£m b·∫£o vi·ªác tr√≠ch xu·∫•t ph·∫ßn d∆∞ (residuals) ƒë·∫°t ƒë·ªô tinh khi·∫øt t·ªëi ∆∞u.
- **Differentiable DAG Discovery (NOTEARS):** S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p t·ªëi ∆∞u h√≥a li√™n t·ª•c ƒë·ªÉ t√¨m ki·∫øm ƒë·ªì th·ªã nh√¢n qu·∫£ ƒëa bi·∫øn, ƒë·∫£m b·∫£o t√≠nh kh√¥ng v√≤ng (Acyclicity) th√¥ng qua c√°c r√†ng bu·ªôc ƒë·∫°i s·ªë ƒë·∫°o h√†m ƒë∆∞·ª£c.
- **Hybrid Independence Testing (HSIC):** K·∫øt h·ª£p Hilbert-Schmidt Independence Criterion l√†m h√†m ph·∫°t (penalty) ƒë·ªÉ c∆∞·ª°ng b·ª©c t√≠nh ƒë·ªôc l·∫≠p nh√¢n qu·∫£ gi·ªØa c√°c bi·∫øn v√† ph·∫ßn d∆∞.

---

## üöÄ S·ª± ti·∫øn h√≥a t·ª´ Base Project (amber0309)

CausalFlow kh√¥ng ch·ªâ k·∫ø th·ª´a m√† c√≤n t√°i c·∫•u tr√∫c to√†n di·ªán d·ª± √°n ANM-MM/GPPOM-HSIC ban ƒë·∫ßu:

| Kh√≠a c·∫°nh | Base Project (amber0309) | **CausalFlow (Ours)** | Gi√° tr·ªã h·ªá th·ªëng |
| :--- | :--- | :--- | :--- |
| **Tri·∫øt l√Ω thi·∫øt k·∫ø** | T·∫≠p h·ª£p c√°c Script nghi√™n c·ª©u | **Unified Machine Learning Engine** | Chuy·ªÉn ƒë·ªïi t·ª´ c√¥ng c·ª• ƒë∆°n l·∫ª th√†nh m·ªôt Framework ho√†n ch·ªânh. |
| **Ki·∫øn tr√∫c m√£ ngu·ªìn** | Ph·∫≥ng & Ph√¢n m·∫£nh | **Ph√¢n l·ªõp Chuy√™n nghi·ªáp (Core/Models/Utils)** | D·ªÖ d√†ng b·∫£o tr√¨, m·ªü r·ªông v√† t√≠ch h·ª£p v√†o c√°c h·ªá th·ªëng kh√°c. |
| **M√¥ h√¨nh h√≥a Nhi·ªÖu** | Gi·∫£ ƒë·ªãnh ƒë∆°n gi·∫£n | **Neural Spline Flows (NSF)** | Kh·∫£ nƒÉng h·ªçc c√°c ph√¢n ph·ªëi nhi·ªÖu phi tuy·∫øn ph·ª©c t·∫°p nh·∫•t. |
| **C·∫•u tr√∫c ƒê·ªì th·ªã** | Gi·ªõi h·∫°n ·ªü song bi·∫øn (Bivariate) | **NOTEARS (Multivariate DAG)** | Kh√°m ph√° c·∫•u tr√∫c c·ªßa h√†ng ch·ª•c bi·∫øn c√πng l√∫c m·ªôt c√°ch ƒë·ªìng b·ªô. |
| **Giao di·ªán l·∫≠p tr√¨nh** | H√†m r·ªùi r·∫°c, g·ªçi th·ªß c√¥ng | **Sklearn-compatible OO API** | Th√¢n thi·ªán v·ªõi l·∫≠p tr√¨nh vi√™n: `model.fit()`, `model.predict()`. |
| **X·ª≠ l√Ω d·ªØ li·ªáu** | Ti·ªÅn x·ª≠ l√Ω t·ªëi gi·∫£n | **SOTA Hybrid Pipeline (IsoForest + QT)** | Lo·∫°i b·ªè nhi·ªÖu sinh h·ªçc, tƒÉng t√≠nh h·ªôi t·ª• cho m√¥ h√¨nh s√¢u. |
| **Ph√¢n t√≠ch n√¢ng cao** | Kh√¥ng h·ªó tr·ª£ | **Counterfactual & Stability Suite** | Cho ph√©p m√¥ ph·ªèng k·ªãch b·∫£n gi·∫£ t∆∞·ªüng "What-if" v√† th·∫©m ƒë·ªãnh k·∫øt qu·∫£. |
| **ƒê·ªô tin c·∫≠y** | Ch·ªâ test tr√™n d·ªØ li·ªáu m√¥ ph·ªèng | **Real-world Sachs Benchmark (70.6%)** | ƒê∆∞·ª£c ki·ªÉm ch·ª©ng tr√™n b·ªô d·ªØ li·ªáu protein th·ª±c t·∫ø kh·∫Øt khe nh·∫•t. |
| **T√†i li·ªáu & ƒê·∫∑c t·∫£** | README ng·∫Øn g·ªçn | **H·ªá th·ªëng ARCH.md & ƒê·∫∑c t·∫£ chi ti·∫øt** | Minh b·∫°ch v·ªÅ thu·∫≠t to√°n v√† c·∫•u tr√∫c s∆° ƒë·ªì ho·∫°t ƒë·ªông. |

---

## üì¶ C√†i ƒë·∫∑t

```bash
pip install git+https://github.com/manhthai1706/CausalFlow.git
```

## üí° H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng (Unified API)

### 1. Kh√°m ph√° h∆∞·ªõng nh√¢n qu·∫£ song bi·∫øn (SOTA Pattern)
T·ª± ƒë·ªông ch·∫°y quy tr√¨nh Hypotheses Testing t√≠ch h·ª£p:
```python
from causalflow import CausalFlow

model = CausalFlow(lda=12.0)
direction = model.predict_direction(pair_data) # Tr·∫£ v·ªÅ 1 (X->Y) ho·∫∑c -1 (Y->X)
```

### 2. Hu·∫•n luy·ªán v√† Suy di·ªÖn ƒêa bi·∫øn
M√¥ h√¨nh t·ª± ƒë·ªông nh·∫≠n di·ªán chi·ªÅu d·ªØ li·ªáu v√† hu·∫•n luy·ªán:
```python
# C√°ch 1: Train ngay khi kh·ªüi t·∫°o
model = CausalFlow(data=data_matrix, epochs=200)

# C√°ch 2: G·ªçi model nh∆∞ m·ªôt h√†m ƒë·ªÉ train
model = CausalFlow()
model(data_matrix, epochs=200)

# Tr√≠ch xu·∫•t ma tr·∫≠n DAG
W_raw, W_binary = model.get_dag_matrix()
```

---

## üìä K·∫øt qu·∫£ Th·ª±c nghi·ªám

Hi·ªáu su·∫•t ƒë∆∞·ª£c ki·ªÉm ch·ª©ng tr√™n b·ªô d·ªØ li·ªáu sinh h·ªçc th·ª±c t·∫ø **Sachs** (Protein Signaling Network), ƒë·∫°t k·∫øt qu·∫£ v∆∞·ª£t tr·ªôi:

- **ƒê·ªô ch√≠nh x√°c x√°c ƒë·ªãnh h∆∞·ªõng (Accuracy): 70.6%** (12/17 c·∫°nh ƒë∆∞·ª£c x√°c ƒë·ªãnh ƒë√∫ng).
- **SHD (Structural Hamming Distance): 5**.
- Kh·∫£ nƒÉng x·ª≠ l√Ω phi tuy·∫øn m·∫°nh m·∫Ω, l·ªçc nhi·ªÖu hi·ªáu qu·∫£ b·∫±ng Isolation Forest v√† Quantile Transformation.

### Ch·ªâ s·ªë hi·ªáu nƒÉng so s√°nh

| Thu·∫≠t to√°n | X·ª≠ l√Ω Phi tuy·∫øn | ƒê·ªô ch√≠nh x√°c (Sachs) | SHD | T√≠nh ·ªïn ƒë·ªãnh |
| :--- | :--- | :--- | :--- | :--- |
| **PC Algorithm** | K√©m | ~50-55% | Cao | Th·∫•p |
| **NOTEARS (Original)** | Trung b√¨nh | ~60% | > 8 | Trung b√¨nh |
| **CausalFlow (Ours)** | **R·∫•t t·ªët (NSF)** | **70.6%** | **5** | **Cao** |

---

## üìö Tham kh·∫£o

- **ANM-MM (amber0309).** [GitHub Repository](https://github.com/amber0309/ANM-MM). (C∆° s·ªü thu·∫≠t to√°n ban ƒë·∫ßu).
- **Zheng, X., et al. (2018).** "DAGs with NO TEARS: Continuous Optimization for Structure Learning." *NeurIPS*.
- **Durkan, C., et al. (2019).** "Neural Spline Flows." *NeurIPS*.
- **Zhang, K., & Hyvarinen, A. (2009).** "On the Identifiability of the Post-Nonlinear Causal Model." *UAI*.
- **Rahimi, A., & Recht, B. (2007).** "Random Features for Large-Scale Kernel Machines." *NeurIPS*. (T·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô HSIC th√¥ng qua RFF).
- **Gretton, A., et al. (2007).** "A Kernel Statistical Test of Independence." *NeurIPS*. (N·ªÅn t·∫£ng c·ªßa c√°c ph√©p th·ª≠ ƒë·ªôc l·∫≠p HSIC).
- **Vaswani, A., et al. (2017).** "Attention Is All You Need." *NeurIPS*. (C∆° ch·∫ø Self-Attention trong l·ªõp MLP ƒë·ªÉ tr·ªçng s·ªë h√≥a ƒë·∫∑c tr∆∞ng).
- **Jang, E., et al. (2016).** "Categorical Reparameterization with Gumbel-Softmax." *ICLR*. (C∆° ch·∫ø ph√¢n c·ª•m c∆° ch·∫ø nh√¢n qu·∫£ c√≥ th·ªÉ ƒë·∫°o h√†m).
- **Kingma, D. P., & Welling, M. (2013).** "Auto-Encoding Variational Bayes." *ICLR*. (Ki·∫øn tr√∫c VAE ƒë·ªÉ ph√°t hi·ªán c∆° c·∫•u ti·ªÅm ·∫©n).
- **He, K., et al. (2016).** "Deep Residual Learning for Image Recognition." *CVPR*. (C∆° ch·∫ø Residual Connections trong kh·ªëi ResBlock).
- **Ba, J. L., et al. (2016).** "Layer Normalization." *arXiv*. (K·ªπ thu·∫≠t chu·∫©n h√≥a l·ªõp ƒë·ªÉ ·ªïn ƒë·ªãnh qu√° tr√¨nh hu·∫•n luy·ªán).
- **Hendrycks, D., & Gimpel, K. (2016).** "Gaussian Error Linear Units (GELUs)." *arXiv*. (H√†m k√≠ch ho·∫°t GELU trong m√¥ h√¨nh MLP).
- **Lim, B., et al. (2021).** "Temporal Fusion Transformers." *International Journal of Forecasting*. (C·∫•u tr√∫c Gated Residual Network - GRN cho vi·ªác ch·ªçn l·ªçc ƒë·∫∑c tr∆∞ng).
- **Loshchilov, I., & Hutter, F. (2017).** "Decoupled Weight Decay Regularization." *ICLR*. (Thu·∫≠t to√°n t·ªëi ∆∞u AdamW s·ª≠ d·ª•ng trong Trainer).
- **Liu, F. T., et al. (2008).** "Isolation Forest." *ICDM*. (S·ª≠ d·ª•ng l·ªçc Outliers trong ti·ªÅn x·ª≠ l√Ω).
- **Pedregosa, F., et al. (2011).** "Scikit-learn: Machine Learning in Python." *JMLR*. (Cung c·∫•p QuantileTransformer).
- **Paszke, A., et al. (2019).** "PyTorch: An Imperative Style, High-Performance Deep Learning Library." *NeurIPS*.

## ‚öñÔ∏è License
D·ª± √°n ƒë∆∞·ª£c ph√°t h√†nh d∆∞·ªõi gi·∫•y ph√©p MIT License.
