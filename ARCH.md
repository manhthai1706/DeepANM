# CausalFlow Architecture

T√†i li·ªáu n√†y cung c·∫•p s∆° ƒë·ªì ho·∫°t ƒë·ªông chi ti·∫øt cho t·ª´ng th√†nh ph·∫ßn trong m√£ ngu·ªìn c·ªßa framework CausalFlow.

---

## üìÇ Th∆∞ m·ª•c `causalflow/core/` (N·ªÅn t·∫£ng thu·∫≠t to√°n)

### 1. `mlp.py` - Ultimate Deep Learning Backbone
ƒê√¢y l√† t·ªáp ph·ª©c t·∫°p nh·∫•t, ch·ªãu tr√°ch nhi·ªám tr√≠ch xu·∫•t th·ª±c th·ªÉ v√† m√¥ h√¨nh h√≥a nhi·ªÖu.

```mermaid
graph TD
    IN[Input X] --> ATT[Attention Layer: Feature Selection]
    ATT --> GRN[Gated Residual Network: GRN]
    GRN --> RB[ResBlocks: Residual Learning]
    
    subgraph Multi-Head_Outputs
        RB --> VAE[VAE Head: mu, log_var for Mechanism Z]
        RB --> NSF[Monotonic Spline: Noise Transformation h_y]
        RB --> REG[Regressor: Probabilistic Output mu_y, var_y]
    end
    
    VAE --> Z[Softmax Z clusters]
    NSF --> HY[Y Transformation]
```

### 2. `gppom_hsic.py` - Core Engine & DAG Learning
ƒêi·ªÅu ph·ªëi vi·ªác h·ªçc ƒë·ªì th·ªã nh√¢n qu·∫£ v√† k·∫øt h·ª£p c√°c h√†m m·∫•t m√°t.

```mermaid
graph TD
    B[Batch Data] --> MLP[Call: mlp.py for Latents]
    MLP --> Z[Mechanism Z]
    
    subgraph DAG_Optimization
        W[W_dag Matrix] --> PEN[Acyclicity Penalty: h_W]
        W --> MASK[Structural Masking]
    end
    
    subgraph Prediction_Flow
        B & MASK --> GP[Random Fourier Features GP]
        GP --> PRED[Y Prediction]
    end
    
    PRED --> MSE[Loss: Regression]
    Z & B --> HSIC1[Loss: FastHSIC Clustering]
    PRED & B --> HSIC2[Loss: Adaptive HSIC PNL]
    
    MSE & PEN & HSIC1 & HSIC2 --> TOTAL[Total Loss & Backward]
```

### 3. `hsic.py` - Statistical Independence Testing
Tri·ªÉn khai c√°c ph√©p th·ª≠ th·ªëng k√™ ƒë·ªÉ x√°c nh·∫≠n quan h·ªá nh√¢n qu·∫£.

```mermaid
graph LR
    subgraph hsic_gam
        A[Data X, Y] --> K[Compute Kernels K, L]
        K --> H[Trace Calculation]
        H --> GAM[Gamma Approximation]
        GAM --> P[p-value / Stat]
    end
    
    subgraph hsic_perm
        A1[Data] --> K1[Kernels]
        K1 --> SHUFFLE[Permutation/Shuffle]
        SHUFFLE --> DIST[Null Distribution]
    end
```

### 4. `kernels.py` - Differentiable Kernel Library
S∆° ƒë·ªì ph√¢n c·∫•p c√°c h√†m nh√¢n c√≥ th·ªÉ ƒë·∫°o h√†m.

```mermaid
graph TD
    K[Base Kernel] --> RBF[RBF / Gaussian]
    K --> MAT[Matern 3/2 & 5/2]
    K --> RQ[Rational Quadratic]
    K --> LIN[Linear / Poly]
    
    subgraph Optimization
        PARAM[log_alpha, log_gamma] --> GRAD[Learnable via SGD]
    end
```

---

## üìÇ Th∆∞ m·ª•c `causalflow/models/` (Giao di·ªán & ·ª®ng d·ª•ng)

### 5. `causalflow.py` - Sklearn-style Wrapper
Giao di·ªán ch√≠nh cho ng∆∞·ªùi d√πng cu·ªëi.

```mermaid
graph TD
    START[CausalFlow Object] --> INIT[Init Dimensions & Device]
    INIT --> FIT[Method: fit]
    
    subgraph FIT_Logic
        FIT --> BIV[Check: Bivariate X, Y?]
        FIT --> MULTI[Check: Multivariate X?]
        BIV & MULTI --> TRAIN[Create: CausalFlowTrainer]
    end
    
    TRAIN --> RESULT[Update History & Weights]
    RESULT --> DAG[Method: get_dag_matrix]
```

### 6. `trainer.py` - Training Orchestrator
Qu·∫£n l√Ω v√≤ng l·∫∑p hu·∫•n luy·ªán v√† l·ªãch tr√¨nh (scheduling).

```mermaid
graph TD
    LOOP[For Epoch in Epochs] --> TEMP[Adjust Temperature: Gumbel-Softmax]
    TEMP --> BATCH[For Batch in DataLoader]
    
    subgraph Batch_Processing
        BATCH --> ZERO[optimizer.zero_grad]
        ZERO --> FORWARD[model.forward]
        FORWARD --> BACK[loss.backward]
        BACK --> STEP[optimizer.step]
    end
    
    STEP --> LOG[Logging: Loss & HSIC Trend]
```

### 7. `analysis.py` - Causal Direction Discovery
L√¥-gic ph√¢n t√≠ch nh√¢n qu·∫£ n√¢ng cao (SOTA 70.6%).

```mermaid
graph TD
    DATA[Raw Data Pair] --> PRE[Standardize / Quantile Transform]
    PRE --> CLEAN[Isolation Forest: Remove Outliers]
    
    subgraph Hypothesis_Testing
        CLEAN --> H1[Test Hypothesis: X -> Y]
        H1 --> LOCK1[Lock W_dag: Force Direction]
        LOCK1 --> SCORE1[Compute HSIC Stability Score 1]
        
        CLEAN --> H2[Test Hypothesis: Y -> X]
        H2 --> LOCK2[Lock W_dag: Force Direction]
        LOCK2 --> SCORE2[Compute HSIC Stability Score 2]
    end
    
    SCORE1 & SCORE2 --> COMP[Compare Scores]
    COMP --> DECIDE[Final Decision: Min Score Wins]

---

## 8. Lu·ªìng ho·∫°t ƒë·ªông t·ªïng th·ªÉ (Overall System Workflow)

S∆° ƒë·ªì d∆∞·ªõi ƒë√¢y m√¥ t·∫£ h√†nh tr√¨nh c·ªßa d·ªØ li·ªáu t·ª´ khi b·∫Øt ƒë·∫ßu cho ƒë·∫øn khi tr√≠ch xu·∫•t ƒë∆∞·ª£c tri th·ª©c nh√¢n qu·∫£:

```mermaid
graph TD
    %% Input Stage
    DATA[D·ªØ li·ªáu quan s√°t] --> PRE[Ti·ªÅn x·ª≠ l√Ω: Quantile + Isolation Forest]
    
    %% Model Initialization
    PRE --> INIT[Kh·ªüi t·∫°o m√¥ h√¨nh CausalFlow]
    
    %% Training Loop
    subgraph Training_Phase [Giai ƒëo·∫°n Hu·∫•n luy·ªán]
        INIT --> FORWARD[Forward Pass: MLP Backbone]
        FORWARD --> LATENT[VAE: Latent Mechanism Discovery]
        LATENT --> GP[Gaussian Process Prediction]
        GP --> REG[T√≠nh MSE Loss]
        
        subgraph Constraints [R√†ng bu·ªôc Nh√¢n qu·∫£]
            W[W_dag Matrix] --> NT[NOTEARS Acyclicity Penalty]
            GP --> RES[Tr√≠ch xu·∫•t Residuals]
            RES --> HSIC[HSIC Independence Penalty]
        end
        
        REG & NT & HSIC --> OPT[AdamW Optimizer Update]
        OPT -->|L·∫∑p l·∫°i| FORWARD
    end
    
    %% Inference Phase
    Training_Phase --> INF[Giai ƒëo·∫°n Suy di·ªÖn / Tr√≠ch xu·∫•t]
    
    subgraph Analysis_Phase [Ph√¢n t√≠ch & K·∫øt lu·∫≠n]
        INF --> DAG_MAT[L·∫•y ma tr·∫≠n DAG t·ª´ W_dag]
        INF --> BIV_TEST[Ki·ªÉm tra h∆∞·ªõng song bi·∫øn Fixed-Structure]
    end
    
    DAG_MAT & BIV_TEST --> OUTPUT[C·∫•u tr√∫c Nh√¢n qu·∫£ cu·ªëi c√πng]
```
```
